{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Embedding Vectors and the RAG Pattern"
      ],
      "metadata": {
        "id": "7l_rj5IU7slV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings Basics"
      ],
      "metadata": {
        "id": "ZHg4-KUf7w6K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SJx-5DqvNd4"
      },
      "outputs": [],
      "source": [
        "# Install OpenAI API\n",
        "!pip install -q openai levenshtein > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Create OpenAI client\n",
        "client = OpenAI(\n",
        "    api_key=userdata.get('openaiKey'),\n",
        ")\n",
        "\n",
        "# Define a helper function to calculate embeddings\n",
        "def get_embedding_vec(input):\n",
        "  \"\"\"Returns the embeddings vector for a given input\"\"\"\n",
        "  return client.embeddings.create(\n",
        "        input=input,\n",
        "        model=\"text-embedding-ada-002\"\n",
        "    ).data[0].embedding"
      ],
      "metadata": {
        "id": "-GeFgmtCxSzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calculate the embedding vector for a sample sentence\n",
        "vec = get_embedding_vec(\"King\")\n",
        "print(vec[:10])\n",
        "\n",
        "# Calculate the magnitude of the vector. I should be 1 as\n",
        "# embedding vectors from OpenAI are always normalized.\n",
        "magnitude = np.linalg.norm(vec)\n",
        "magnitude"
      ],
      "metadata": {
        "id": "AkSs4jICv-t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from Levenshtein import distance\n",
        "\n",
        "# Compare two sentences with the same meaning, but in different languages\n",
        "s1 = \"The king is dead, long live the king\"\n",
        "s2 = \"Der König ist tot, lang lebe der König\"\n",
        "print(f\"a) Similarity: {np.dot(get_embedding_vec(s1), get_embedding_vec(s2)): .4f}, Levenshtein distance: {distance(s1, s2)}\")\n",
        "\n",
        "s1 = \"The king who is alive is better than a dead one\"\n",
        "s2 = \"Ein König der lebt, ist besser als ein toter König\"\n",
        "print(f\"b) Similarity: {np.dot(get_embedding_vec(s1), get_embedding_vec(s2)): .4f}, Levenshtein distance: {distance(s1, s2)}\")\n",
        "\n",
        "# Compare two sentences with similar words, but different meaning\n",
        "s1 = \"The king is dead, long live the king\"\n",
        "s2 = \"The king is alive, that's better than a dead king\"\n",
        "print(f\"c) Similarity: {np.dot(get_embedding_vec(s1), get_embedding_vec(s2)): .4f}, Levenshtein distance: {distance(s1, s2)}\")\n",
        "\n",
        "# Compare two sentences with similar words, but different meaning and languages\n",
        "s1 = \"The king is dead, long live the king\"\n",
        "s2 = \"Ein König der lebt, ist besser als ein toter König\"\n",
        "print(f\"d) Similarity: {np.dot(get_embedding_vec(s1), get_embedding_vec(s2)): .4f}, Levenshtein distance: {distance(s1, s2)}\")\n"
      ],
      "metadata": {
        "id": "yw161KFrcTRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Search"
      ],
      "metadata": {
        "id": "JSVFoT47707S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Array with descriptions about cities. They could be used in e.g. a travel\n",
        "# agency to find suitable spots for vacations\n",
        "cities = []\n",
        "cities.append(\"Emeraldine: A bustling metropolis surrounded by lush forests, known for its towering skyscrapers and vibrant night markets.\")\n",
        "cities.append(\"Solara: A small, sun-drenched coastal town famous for its golden beaches, seafood cuisine, and laid-back lifestyle.\")\n",
        "cities.append(\"Nebulae: A futuristic city with floating buildings and neon lights, renowned for its advanced technology and AI-driven services.\")\n",
        "cities.append(\"Auroria: A serene mountain city, hidden in misty peaks, with ancient monasteries and breathtaking hiking trails.\")\n",
        "cities.append(\"Thalassa: An island city with a rich maritime history, surrounded by crystal clear waters, perfect for scuba diving and sailing.\")\n",
        "cities.append(\"Cinderpeak: A city built around an active volcano, known for its unique architecture, geothermal energy, and vibrant arts scene.\")\n",
        "cities.append(\"Vespera: A city that never sleeps, with a bustling nightlife, cultural festivals, and a diverse culinary scene, under a starlit sky.\")\n",
        "cities.append(\"Windmere: A small town on the plains, famous for its windmills, open fields, and a tight-knit community with traditional values.\")\n",
        "cities.append(\"Polaria: An isolated city in the far north, known for its ice castles, aurora borealis views, and resilient, warm-hearted residents.\")\n",
        "cities.append(\"Glimmerdale: A city in a valley, illuminated by bioluminescent plants, known for its sustainable living and harmony with nature.\")\n"
      ],
      "metadata": {
        "id": "a4VWvep21bI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's calculate the embedding vectors of all cities.\n",
        "# NOTE that in real-world applications, you would store the embeddings\n",
        "# in a vector DB like Pinecone, Qdrant, Azure Search, etc.\n",
        "embeddings = []\n",
        "for city in cities:\n",
        "  embeddings.append((city, get_embedding_vec(city)))"
      ],
      "metadata": {
        "id": "O0k1GHPP1k09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter the search text of the customer how is looking for a vacation spot.\n",
        "#query = \"For vacation, I want to go hiking in the mountains. I want to really feel nature.\"\n",
        "#query = \"In my vacation, I want to party, party, party!\"\n",
        "#query = \"I want to travel to an underwater city\"\n",
        "query = \"What is the name of Bart Simpson's sister?\"\n",
        "\n",
        "# Calculate the embedding vector of the search text\n",
        "query_embedding = get_embedding_vec(query)"
      ],
      "metadata": {
        "id": "pILrd_Sp14Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "sorted_result = []\n",
        "\n",
        "# Iterate over all cities and calculate the similarity (dot product) of\n",
        "# the city description and the search text.\n",
        "for city, embedding in embeddings:\n",
        "  similarity = np.dot(embedding, query_embedding)\n",
        "  sorted_result.append((city, embedding, similarity))\n",
        "\n",
        "# We sort the result descending based on the similarity so that the top\n",
        "# elements are probably more relevant than the last ones.\n",
        "sorted_result = sorted(sorted_result, key=lambda x: x[2], reverse=True)\n",
        "for tuple in sorted_result:\n",
        "    print(tuple[2], tuple[0])\n"
      ],
      "metadata": {
        "id": "zulIp7OV2I3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "cities = [item[0].split(\":\", 1)[0] for item in sorted_result]\n",
        "similarities = [item[2] for item in sorted_result]\n",
        "\n",
        "# Visualize the sorted result in a bar chart\n",
        "plt.bar(cities, similarities, color='lime')\n",
        "plt.ylabel('Similarity')\n",
        "plt.ylim(\n",
        "    math.floor(min(similarities) * 100) / 100,\n",
        "    math.ceil(max(similarities) * 100) / 100)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9KjbI6Ty5qav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Response"
      ],
      "metadata": {
        "id": "2Pg0KxLJ9bRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from string import Template\n",
        "\n",
        "t = Template(\"\"\"\n",
        "You are a helpful assistant in a travel agency. Customers are describing\n",
        "what they want to do in their vacation. Make suggestions based on the\n",
        "city descriptions provided below. ONLY use the provided city descriptions.\n",
        "Do NOT use other information sources.\n",
        "\n",
        "If you cannot generate a meaningful answer based on the given city description,\n",
        "say \"Sorry, I cannot help\". If the user's input is not related to finding\n",
        "a travel location, say \"Sorry, I can only help with vacation locations\".\n",
        "\n",
        "===========\n",
        "$options\n",
        "===========\n",
        "\"\"\")\n",
        "\n",
        "system_prompt = t.substitute(options = \"\\n\\n\".join([item[0] for item in sorted_result[:3]]))\n",
        "print(system_prompt)\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt,\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": query,\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-4-1106-preview\",\n",
        ")\n",
        "chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "ujBF_UquzwYx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}